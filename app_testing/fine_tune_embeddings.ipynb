{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Setting up the vector database...\n",
      "✅ Vector database initialized\n",
      "\n",
      "Step 2: Loading profiles from CSV...\n",
      "✅ Loaded 20 profiles from CSV\n",
      "\n",
      "Step 3: Creating LangChain documents for embedding...\n",
      "\n",
      "Sample document for embedding:\n",
      "--------------------------------------------------\n",
      "Document ID (UUID4): 691a9329-8d0e-4eaa-839d-58bf312baca4\n",
      "Name: Keith Teare\n",
      "Location: Palo Alto, California, United States, United States\n",
      "About: I am a founder and CEO at SignalRank, a technology company that uses data intelligence to form a partner network with top-performing managers. We have built a financial instrument that captures top decile value creation that is exclusive to the best early stage companies. I have over 40 years of experience in digital technology, as a founder, CTO, or CEO, in various domains, such as internet services, keywords...\n",
      "--------------------------------------------------\n",
      "✅ Created 20 documents with UUID4 identifiers\n",
      "\n",
      "Step 4: Creating LangChain vector store and retriever...\n",
      "Processing batch 1/2 (documents 1-10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1113012/563087777.py:27: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n",
      "/tmp/ipykernel_1113012/563087777.py:121: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
      "  vector_store = Qdrant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 1 second before next batch...\n",
      "Processing batch 2/2 (documents 11-20)...\n",
      "✅ Finished processing 20 documents with UUID4 identifiers\n",
      "✅ Created LangChain retriever\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.schema.document import Document\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import uuid  # Import UUID module\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "COLLECTION_NAME = \"founders\"\n",
    "VECTOR_DIM = 1536  # OpenAI embedding dimension\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "BATCH_SIZE = 10  # Process profiles in smaller batches to avoid rate limits\n",
    "\n",
    "print(\"Step 1: Setting up the vector database...\")\n",
    "# Initialize Qdrant client (in-memory)\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Create collection\n",
    "client.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(size=VECTOR_DIM, distance=Distance.COSINE)\n",
    ")\n",
    "print(\"✅ Vector database initialized\")\n",
    "\n",
    "print(\"\\nStep 2: Loading profiles from CSV...\")\n",
    "# Update this path to your CSV file location\n",
    "csv_path = \"specter-people-db--export_small.csv\"  # Update this to your CSV file path\n",
    "df = pd.read_csv(csv_path)\n",
    "profiles = df.to_dict('records')\n",
    "print(f\"✅ Loaded {len(profiles)} profiles from CSV\")\n",
    "\n",
    "print(\"\\nStep 3: Creating LangChain documents for embedding...\")\n",
    "# Initialize the embeddings model\n",
    "embeddings_model = OpenAIEmbeddings(model=EMBEDDING_MODEL)\n",
    "\n",
    "# Create LangChain documents with UUID4 IDs\n",
    "documents = []\n",
    "document_ids = []  # Store UUIDs separately\n",
    "\n",
    "for p in profiles:\n",
    "    # Generate a UUID4 for this document\n",
    "    doc_id = str(uuid.uuid4())\n",
    "    \n",
    "    # Build a text representation including key fields\n",
    "    text_parts = []\n",
    "    \n",
    "    # Add core identity information\n",
    "    if p.get('Full Name'): text_parts.append(f\"Name: {p.get('Full Name')}\")\n",
    "    if p.get('Current Position'): text_parts.append(f\"Position: {p.get('Current Position Title')}\")\n",
    "    if p.get('Company'): text_parts.append(f\"Company: {p.get('Current Position Company Name')}\")\n",
    "    if p.get('Location'): text_parts.append(f\"Location: {p.get('Location')}\")\n",
    "    \n",
    "    # Add contact and social media information\n",
    "    if p.get('LinkedIn'): text_parts.append(f\"LinkedIn: {p.get('LinkedIn - URL')}\")\n",
    "    if p.get('Twitter'): text_parts.append(f\"Twitter: {p.get('Twitter - URL')}\")\n",
    "    if p.get('Website'): text_parts.append(f\"Website: {p.get('Website - URL')}\")\n",
    "    if p.get('Email'): text_parts.append(f\"Email: {p.get('Email')}\")\n",
    "    \n",
    "    # Add detailed professional information\n",
    "    if p.get('About'): text_parts.append(f\"About: {p.get('About')}\")\n",
    "    if p.get('Skills'): text_parts.append(f\"Skills: {p.get('Skills')}\")\n",
    "    if p.get('Experience'): text_parts.append(f\"Experience: {p.get('Experience')}\")\n",
    "    if p.get('Education'): text_parts.append(f\"Education: {p.get('Education')}\")\n",
    "    \n",
    "    # Add any industry or sector information\n",
    "    if p.get('Industry'): text_parts.append(f\"Industry: {p.get('Industry')}\")\n",
    "    if p.get('Sector'): text_parts.append(f\"Sector: {p.get('Sector')}\")\n",
    "    \n",
    "    # Add any entrepreneurial information\n",
    "    if p.get('Previous Startups'): text_parts.append(f\"Previous Startups: {p.get('Previous Startups')}\")\n",
    "    if p.get('Funding History'): text_parts.append(f\"Funding History: {p.get('Funding History')}\")\n",
    "    \n",
    "    # Add any additional fields that might be in the CSV\n",
    "    for key, value in p.items():\n",
    "        if (key not in ['Full Name', 'Current Position', 'Company', 'Location', \n",
    "                       'LinkedIn', 'Twitter', 'Website', 'Email',\n",
    "                       'About', 'Skills', 'Experience', 'Education', \n",
    "                       'Industry', 'Sector', 'Previous Startups', 'Funding History'] \n",
    "            and value and str(value).lower() != 'nan'):\n",
    "            text_parts.append(f\"{key}: {value}\")\n",
    "    \n",
    "    # Join all parts with newlines for better separation\n",
    "    text = \"\\n\".join(text_parts)\n",
    "    \n",
    "    # Add the UUID to the metadata\n",
    "    p['doc_id'] = doc_id\n",
    "    \n",
    "    # Create a LangChain Document with the text and metadata\n",
    "    document = Document(\n",
    "        page_content=text,\n",
    "        metadata=p  # Store the original profile as metadata (now includes doc_id)\n",
    "    )\n",
    "    documents.append(document)\n",
    "    document_ids.append(doc_id)\n",
    "\n",
    "# Print a sample document with its UUID\n",
    "if documents:\n",
    "    print(\"\\nSample document for embedding:\")\n",
    "    print(\"-\" * 50)\n",
    "    sample_doc = documents[0]\n",
    "    sample_id = document_ids[0]\n",
    "    print(f\"Document ID (UUID4): {sample_id}\")\n",
    "    if len(sample_doc.page_content) > 500:\n",
    "        print(sample_doc.page_content[:500] + \"...\")\n",
    "    else:\n",
    "        print(sample_doc.page_content)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"✅ Created {len(documents)} documents with UUID4 identifiers\")\n",
    "\n",
    "print(\"\\nStep 4: Creating LangChain vector store and retriever...\")\n",
    "# First, create the vector store with the existing client\n",
    "vector_store = Qdrant(\n",
    "    client=client,  # Use the existing client\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embeddings=embeddings_model\n",
    ")\n",
    "\n",
    "# Process in batches to avoid rate limits\n",
    "total_documents = len(documents)\n",
    "num_batches = (total_documents + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "# Process documents in batches\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min(start_idx + BATCH_SIZE, total_documents)\n",
    "    \n",
    "    print(f\"Processing batch {batch_idx+1}/{num_batches} (documents {start_idx+1}-{end_idx})...\")\n",
    "    \n",
    "    batch_docs = documents[start_idx:end_idx]\n",
    "    batch_ids = document_ids[start_idx:end_idx]\n",
    "    \n",
    "    try:\n",
    "        # Extract texts and metadatas from documents\n",
    "        texts = [doc.page_content for doc in batch_docs]\n",
    "        metadatas = [doc.metadata for doc in batch_docs]\n",
    "        \n",
    "        # Add texts to the vector store with their UUIDs\n",
    "        vector_store.add_texts(\n",
    "            texts=texts,\n",
    "            metadatas=metadatas,\n",
    "            ids=batch_ids  # Use UUID4 strings as IDs\n",
    "        )\n",
    "        \n",
    "        # Sleep to avoid rate limits\n",
    "        if batch_idx < num_batches - 1:\n",
    "            print(\"Waiting 1 second before next batch...\")\n",
    "            time.sleep(1)  # 1 second delay between batches\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_idx+1}/{num_batches}: {str(e)}\")\n",
    "        # Continue with next batch\n",
    "\n",
    "print(f\"✅ Finished processing {total_documents} documents with UUID4 identifiers\")\n",
    "\n",
    "# Create a retriever from the vector store\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",  # Options: \"similarity\", \"mmr\"\n",
    "    search_kwargs={\"k\": 3}     # Return top 3 results\n",
    ")\n",
    "\n",
    "print(\"✅ Created LangChain retriever\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUERY:\n",
    "{question}\n",
    "\n",
    "You are a helpful assistant. Use the available context to answer the question. If you can't answer the question, say you don't know.\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | openai_chat_model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the profiles available, here are two founders who may be a good fit for a startup that is building a platform for AI agents to collaborate on tasks:\\n\\n1. **Mark Goldenson**\\n   - **Current Position**: Investor & Advisor at Regrello\\n   - **Experience**: Mark has extensive experience in building products and helping founders, particularly in AI. His background includes roles at Google where he led product management and built AI-powered solutions.\\n   - **About**: He focuses on technology and entrepreneurship, making him a valuable asset in the development of AI products.\\n   - **LinkedIn**: [Mark Goldenson](https://www.linkedin.com/in/goldenson)\\n\\n2. **Alexey Skutin**\\n   - **Current Position**: Founder at AideAI\\n   - **Experience**: With a Ph.D. and over 20 years in the IT industry, Alexey specializes in enterprise, security, and SaaS/PaaS software. His work at AideAI involves creating AI solutions, which aligns closely with the startup's mission of AI agent collaboration.\\n   - **About**: He has a proven track record of scaling teams and launching innovative products in the tech industry.\\n   - **LinkedIn**: [Alexey Skutin](https://www.linkedin.com/in/alexey-skutin-ph-d-6b25356)\\n\\nThese individuals have relevant experience and expertise in AI and technology, making them likely candidates for collaboration in your startup.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"question\" : \"can you help me find founders who would be a good fit for a startup that is building a platform for AI agents to collaborate on tasks?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "os.environ[\"RAGAS_APP_TOKEN\"] = getpass(\"Please enter your Ragas API key!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ngiometti/aie5/code/midtermv2/.venv/lib/python3.13/site-packages/pysbd/segmenter.py:66: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  for match in re.finditer('{0}\\s*'.format(re.escape(sent)), self.original_text):\n",
      "/home/ngiometti/aie5/code/midtermv2/.venv/lib/python3.13/site-packages/pysbd/lang/arabic.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  txt = re.sub('(?<={0})\\.'.format(am), '∯', txt)\n",
      "/home/ngiometti/aie5/code/midtermv2/.venv/lib/python3.13/site-packages/pysbd/lang/persian.py:29: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  txt = re.sub('(?<={0})\\.'.format(am), '∯', txt)\n"
     ]
    }
   ],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 750,\n",
    "    chunk_overlap  = 20,\n",
    "    length_function = len\n",
    ")\n",
    "training_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_set = set()\n",
    "\n",
    "for document in training_documents:\n",
    "  id = str(uuid.uuid4())\n",
    "  while id in id_set:\n",
    "    id = uuid.uuid4()\n",
    "  id_set.add(id)\n",
    "  document.metadata[\"id\"] = id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 393\n",
      "Validation set size: 60\n",
      "Test set size: 60\n",
      "Total: 513\n"
     ]
    }
   ],
   "source": [
    "# For a dataset of 513 documents\n",
    "# Maintaining approximately the same proportions (76.5% / 11.8% / 11.8%)\n",
    "\n",
    "# Calculate the number of documents for validation and test sets\n",
    "val_size = int(513 * 0.118)  # ~60 documents\n",
    "test_size = int(513 * 0.118)  # ~60 documents\n",
    "train_size = 513 - val_size - test_size  # ~393 documents\n",
    "\n",
    "# Create the splits\n",
    "training_split_documents = training_documents[:train_size]\n",
    "val_split_documents = training_documents[train_size:train_size + val_size]\n",
    "test_split_documents = training_documents[train_size + val_size:]\n",
    "\n",
    "# Print the sizes to verify\n",
    "print(f\"Training set size: {len(training_split_documents)}\")\n",
    "print(f\"Validation set size: {len(val_split_documents)}\")\n",
    "print(f\"Test set size: {len(test_split_documents)}\")\n",
    "print(f\"Total: {len(training_split_documents) + len(val_split_documents) + len(test_split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "qa_chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_prompt = \"\"\"\\\n",
    "Given the following context, you must generate questions based on only the provided context.\n",
    "\n",
    "You are to generate {n_questions} questions which should be provided in the following format:\n",
    "\n",
    "1. QUESTION #1\n",
    "2. QUESTION #2\n",
    "...\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_chain = qa_prompt_template | qa_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def process_document(document, n_questions):\n",
    "    questions_generated = await question_generation_chain.ainvoke({\"context\": document.page_content, \"n_questions\": n_questions})\n",
    "    \n",
    "    doc_questions = {}\n",
    "    doc_relevant_docs = {}\n",
    "    \n",
    "    for question in questions_generated.content.split(\"\\n\"):\n",
    "        question_id = str(uuid.uuid4())\n",
    "        doc_questions[question_id] = \"\".join(question.split(\".\")[1:]).strip()\n",
    "        doc_relevant_docs[question_id] = [document.metadata[\"id\"]]\n",
    "    \n",
    "    return doc_questions, doc_relevant_docs\n",
    "\n",
    "async def create_questions(documents, n_questions):\n",
    "    tasks = [process_document(doc, n_questions) for doc in documents]\n",
    "    \n",
    "    questions = {}\n",
    "    relevant_docs = {}\n",
    "    \n",
    "    for task in tqdm(asyncio.as_completed(tasks), total=len(documents), desc=\"Processing documents\"):\n",
    "        doc_questions, doc_relevant_docs = await task\n",
    "        questions.update(doc_questions)\n",
    "        relevant_docs.update(doc_relevant_docs)\n",
    "    return questions, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 393/393 [00:25<00:00, 15.45it/s] \n"
     ]
    }
   ],
   "source": [
    "training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 60/60 [00:07<00:00,  8.50it/s]\n"
     ]
    }
   ],
   "source": [
    "val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 60/60 [00:04<00:00, 14.14it/s]\n"
     ]
    }
   ],
   "source": [
    "test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n",
    "\n",
    "train_dataset = {\n",
    "    \"questions\" : training_questions,\n",
    "    \"relevant_contexts\" : training_relevant_contexts,\n",
    "    \"corpus\" : training_corpus\n",
    "}\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(train_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n",
    "\n",
    "val_dataset = {\n",
    "    \"questions\" : val_questions,\n",
    "    \"relevant_contexts\" : val_relevant_contexts,\n",
    "    \"corpus\" : val_corpus\n",
    "}\n",
    "\n",
    "with open(\"val_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n",
    "\n",
    "test_dataset = {\n",
    "    \"questions\" : test_questions,\n",
    "    \"relevant_contexts\" : test_relevant_contexts,\n",
    "    \"corpus\" : train_corpus\n",
    "}\n",
    "\n",
    "with open(\"test_dataset.jsonl\", \"w\") as f:\n",
    "  json.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n",
    "model = SentenceTransformer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_dataset['corpus']\n",
    "queries = train_dataset['questions']\n",
    "relevant_docs = train_dataset['relevant_contexts']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    doc_id = relevant_docs[query_id][0]\n",
    "    text = corpus[doc_id]\n",
    "    example = InputExample(texts=[query, text])\n",
    "    examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    examples, batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "matryoshka_dimensions = [768, 512, 256, 128, 64]\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "corpus = val_dataset['corpus']\n",
    "queries = val_dataset['questions']\n",
    "relevant_docs = val_dataset['relevant_contexts']\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/8fdjdcxl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb87e55a7b0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de59bf0eb381414892a8251a0037a4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [790/790 35:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cosine Accuracy@1</th>\n",
       "      <th>Cosine Accuracy@3</th>\n",
       "      <th>Cosine Accuracy@5</th>\n",
       "      <th>Cosine Accuracy@10</th>\n",
       "      <th>Cosine Precision@1</th>\n",
       "      <th>Cosine Precision@3</th>\n",
       "      <th>Cosine Precision@5</th>\n",
       "      <th>Cosine Precision@10</th>\n",
       "      <th>Cosine Recall@1</th>\n",
       "      <th>Cosine Recall@3</th>\n",
       "      <th>Cosine Recall@5</th>\n",
       "      <th>Cosine Recall@10</th>\n",
       "      <th>Cosine Ndcg@10</th>\n",
       "      <th>Cosine Mrr@10</th>\n",
       "      <th>Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.891719</td>\n",
       "      <td>0.863968</td>\n",
       "      <td>0.865364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.908024</td>\n",
       "      <td>0.882500</td>\n",
       "      <td>0.883472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.193333</td>\n",
       "      <td>0.099167</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>0.926481</td>\n",
       "      <td>0.904861</td>\n",
       "      <td>0.905075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.909124</td>\n",
       "      <td>0.886111</td>\n",
       "      <td>0.887604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.910017</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.889003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.921426</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.904089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.911044</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.890263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.916104</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.897044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.910847</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.890002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.914500</td>\n",
       "      <td>0.893750</td>\n",
       "      <td>0.895011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.895479</td>\n",
       "      <td>0.865509</td>\n",
       "      <td>0.866117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.901882</td>\n",
       "      <td>0.876389</td>\n",
       "      <td>0.877852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.900791</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.876440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.898031</td>\n",
       "      <td>0.871528</td>\n",
       "      <td>0.873036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.903615</td>\n",
       "      <td>0.876620</td>\n",
       "      <td>0.877476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.902090</td>\n",
       "      <td>0.874306</td>\n",
       "      <td>0.874997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.897733</td>\n",
       "      <td>0.868552</td>\n",
       "      <td>0.869339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.897584</td>\n",
       "      <td>0.868403</td>\n",
       "      <td>0.869180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.905917</td>\n",
       "      <td>0.879514</td>\n",
       "      <td>0.880265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.900539</td>\n",
       "      <td>0.872454</td>\n",
       "      <td>0.873190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.903884</td>\n",
       "      <td>0.876885</td>\n",
       "      <td>0.877632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.904975</td>\n",
       "      <td>0.878274</td>\n",
       "      <td>0.878990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.905166</td>\n",
       "      <td>0.878472</td>\n",
       "      <td>0.879208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.902090</td>\n",
       "      <td>0.874306</td>\n",
       "      <td>0.875065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.322222</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.098333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.901900</td>\n",
       "      <td>0.874107</td>\n",
       "      <td>0.874879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(loader, train_loss)],\n",
    "    epochs=EPOCHS,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='finetuned_arctic_ft',\n",
    "    show_progress_bar=True,\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecd70ceb7934eb0b31ba3f63360dd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_username = \"ngiometti\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8220ce61d5614e8ea8d7cc62e7b7bbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/ngiometti/legal-ft-3/commit/7b3153ca1e1cbfe487564581ed2a67b41701f3d8'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(f\"{hf_username}/legal-ft-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic_ft and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97078309827a4673a7a54698bdf0507a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccae5318f14f4daeb9a85d7552116677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48f3ac68fb644f394d0b574f9c4d359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary' already exists in node '183070'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cac9da56db14b84838d101e12c13726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c8a3d1146a496ebf80dbd3a9b65717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary_embedding' already exists in node '183070'. Skipping!\n",
      "unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-LsdVv2bXqJ9TPrSXuJ0Tazh3 on tokens per min (TPM): Limit 30000, Used 29748, Requested 3047. Please try again in 5.59s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-LsdVv2bXqJ9TPrSXuJ0Tazh3 on tokens per min (TPM): Limit 30000, Used 29116, Requested 3775. Please try again in 5.782s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-LsdVv2bXqJ9TPrSXuJ0Tazh3 on tokens per min (TPM): Limit 30000, Used 29005, Requested 3502. Please try again in 5.014s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-LsdVv2bXqJ9TPrSXuJ0Tazh3 on tokens per min (TPM): Limit 30000, Used 28921, Requested 3544. Please try again in 4.93s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-LsdVv2bXqJ9TPrSXuJ0Tazh3 on tokens per min (TPM): Limit 30000, Used 28166, Requested 5001. Please try again in 6.334s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "unable to apply transformation: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-LsdVv2bXqJ9TPrSXuJ0Tazh3 on tokens per min (TPM): Limit 30000, Used 28492, Requested 5021. Please try again in 7.026s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400e9b3cdca24300b4481bd9c3b6a353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a24bfe1a5054e549dcbaba96040d5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114a7f0561fa4de49d913ba5e7db776c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dff8e881014453b9d7753cb71c0c925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm=generator_llm, embedding_model=finetune_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(documents, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What role does the University of Kent in Ameri...</td>\n",
       "      <td>[Experience\",\"Consumer Internet\",\"Digital Mark...</td>\n",
       "      <td>The individual serves as a volunteer director ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What Keith Teare do in United States?</td>\n",
       "      <td>[Name: Keith Teare Location: Palo Alto, Califo...</td>\n",
       "      <td>Keith Teare is a founder and CEO at SignalRank...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how taiwan fit into the strategy for realnames...</td>\n",
       "      <td>[about the IP Communications space, and partic...</td>\n",
       "      <td>RealNames made deals across 12 countries, incl...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what u know bout University of Kent and what k...</td>\n",
       "      <td>[Education\",\"Hardware\",\"Internet Services\",\"Te...</td>\n",
       "      <td>Keith Teare was presented with an honorary doc...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As an International Finance and Strategy Exper...</td>\n",
       "      <td>[Experience: [{\"Company Name\":\"Start-up-Chris-...</td>\n",
       "      <td>Polkadot is a blockspace ecosystem designed fo...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can an aspiring tech entrepreneur leverage...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nName: Anthony Kelani Location: Los...</td>\n",
       "      <td>An aspiring tech entrepreneur can learn from A...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How has being featured in Forbes and other maj...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWANT TO GET IN TOUCH? Shoot me a D...</td>\n",
       "      <td>Being featured in major publications such as F...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What be the connection between OPEAR and Opera...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nHighlights: [\"fortune_500_experien...</td>\n",
       "      <td>The connection between OPEAR and Operam in ter...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How did Poornima Vijayashanker and Jeff Pressm...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nSkills: [\"Software Development\",\"P...</td>\n",
       "      <td>Poornima Vijayashanker, as the Co-Founder and ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What role did Oliver Walsh play in the growth ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nName: Oliver Walsh Location: Los A...</td>\n",
       "      <td>Oliver Walsh served as the CMO and Board Direc...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What role does the University of Kent in Ameri...   \n",
       "1              What Keith Teare do in United States?   \n",
       "2  how taiwan fit into the strategy for realnames...   \n",
       "3  what u know bout University of Kent and what k...   \n",
       "4  As an International Finance and Strategy Exper...   \n",
       "5  How can an aspiring tech entrepreneur leverage...   \n",
       "6  How has being featured in Forbes and other maj...   \n",
       "7  What be the connection between OPEAR and Opera...   \n",
       "8  How did Poornima Vijayashanker and Jeff Pressm...   \n",
       "9  What role did Oliver Walsh play in the growth ...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Experience\",\"Consumer Internet\",\"Digital Mark...   \n",
       "1  [Name: Keith Teare Location: Palo Alto, Califo...   \n",
       "2  [about the IP Communications space, and partic...   \n",
       "3  [Education\",\"Hardware\",\"Internet Services\",\"Te...   \n",
       "4  [Experience: [{\"Company Name\":\"Start-up-Chris-...   \n",
       "5  [<1-hop>\\n\\nName: Anthony Kelani Location: Los...   \n",
       "6  [<1-hop>\\n\\nWANT TO GET IN TOUCH? Shoot me a D...   \n",
       "7  [<1-hop>\\n\\nHighlights: [\"fortune_500_experien...   \n",
       "8  [<1-hop>\\n\\nSkills: [\"Software Development\",\"P...   \n",
       "9  [<1-hop>\\n\\nName: Oliver Walsh Location: Los A...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The individual serves as a volunteer director ...   \n",
       "1  Keith Teare is a founder and CEO at SignalRank...   \n",
       "2  RealNames made deals across 12 countries, incl...   \n",
       "3  Keith Teare was presented with an honorary doc...   \n",
       "4  Polkadot is a blockspace ecosystem designed fo...   \n",
       "5  An aspiring tech entrepreneur can learn from A...   \n",
       "6  Being featured in major publications such as F...   \n",
       "7  The connection between OPEAR and Operam in ter...   \n",
       "8  Poornima Vijayashanker, as the Co-Founder and ...   \n",
       "9  Oliver Walsh served as the CMO and Board Direc...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_specific_query_synthesizer  \n",
       "7  multi_hop_specific_query_synthesizer  \n",
       "8  multi_hop_specific_query_synthesizer  \n",
       "9  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset uploaded! View at https://app.ragas.io/dashboard/alignment/testset/fe27a246-a89e-4779-8f69-435d3c6c2a83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/testset/fe27a246-a89e-4779-8f69-435d3c6c2a83'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "  retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "  return {\"context\" : retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "  docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "  messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
    "  response = llm.invoke(messages)\n",
    "  return {\"response\" : response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class State(TypedDict):\n",
    "  question: str\n",
    "  context: List[Document]\n",
    "  response: str\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\" : \"Who has the best profile suited to founding an AI startup?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, both Christopher Obereder and Mark Goldenson have strong profiles suited to founding an AI startup, but they each bring different strengths:\\n\\n### Christopher Obereder\\n- **Experience**: Over 12 years in tech, with successful exits of four companies and a deep involvement in venture capital, notably through his firm, Start-Up-Chris Ventures.\\n- **Recognition**: Featured in Forbes 30 Under 30, indicating significant achievement in a short period.\\n- **Skills**: Expertise in growth hacking and fundraising, with a proven track record of raising over $360 million for startups.\\n- **Network**: An extensive investment portfolio with notable companies like Coinbase, Airbnb, and more, showing strong connections in the industry.\\n- **Vision**: Driven by a passion for transformative solutions, indicating a strong entrepreneurial spirit.\\n\\n### Mark Goldenson\\n- **Experience**: 28 years in technology and startups, with a focus on artificial intelligence, health care, and product development.\\n- **Past Roles**: Extensive experience at Google as a group product manager focused on AI projects, providing him with firsthand knowledge of AI technologies and product management.\\n- **Expertise in AI**: He has a direct involvement in developing AI products, particularly in large language models and other AI innovations.\\n- **Networking**: His role at Regrello, alongside advising numerous startups, showcases a commitment to supporting founders and innovation.\\n\\n### Conclusion\\nIf the focus is purely on experience in AI and direct product development within that domain, **Mark Goldenson** would likely be the stronger candidate for founding an AI startup. His extensive experience in AI development at Google and long history in the tech industry gives him a solid foundation for startup leadership in AI.\\n\\nHowever, **Christopher Obereder** could also be a great candidate due to his breadth of entrepreneurial experience, success in fundraising, and growth strategies. His profile suits someone looking to lead a startup with a focus on rapid growth and scaling.\\n\\nIn summary, **Mark Goldenson** is better positioned for an AI startup with a technical and product-centric approach, while **Christopher Obereder** is well-suited for a startup that may require robust fundraising and growth strategies.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_row in dataset:\n",
    "  response = graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What role does the University of Kent in Ameri...</td>\n",
       "      <td>[Name: Christian Busch\\nLocation: Los Angeles,...</td>\n",
       "      <td>[Experience\",\"Consumer Internet\",\"Digital Mark...</td>\n",
       "      <td>The University of Kent in America plays a role...</td>\n",
       "      <td>The individual serves as a volunteer director ...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What Keith Teare do in United States?</td>\n",
       "      <td>[Name: Keith Teare\\nLocation: Palo Alto, Calif...</td>\n",
       "      <td>[Name: Keith Teare Location: Palo Alto, Califo...</td>\n",
       "      <td>Keith Teare is based in Palo Alto, California,...</td>\n",
       "      <td>Keith Teare is a founder and CEO at SignalRank...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>how taiwan fit into the strategy for realnames...</td>\n",
       "      <td>[Name: Alan Chiu\\nLocation: Palo Alto, Califor...</td>\n",
       "      <td>[about the IP Communications space, and partic...</td>\n",
       "      <td>Taiwan fit into the strategy for RealNames dur...</td>\n",
       "      <td>RealNames made deals across 12 countries, incl...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what u know bout University of Kent and what k...</td>\n",
       "      <td>[Name: Keith Teare\\nLocation: Palo Alto, Calif...</td>\n",
       "      <td>[Education\",\"Hardware\",\"Internet Services\",\"Te...</td>\n",
       "      <td>Keith Teare has an honorary doctorate from the...</td>\n",
       "      <td>Keith Teare was presented with an honorary doc...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As an International Finance and Strategy Exper...</td>\n",
       "      <td>[Name: Keith Teare\\nLocation: Palo Alto, Calif...</td>\n",
       "      <td>[Experience: [{\"Company Name\":\"Start-up-Chris-...</td>\n",
       "      <td>Polkadot significantly contributes to the adva...</td>\n",
       "      <td>Polkadot is a blockspace ecosystem designed fo...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can an aspiring tech entrepreneur leverage...</td>\n",
       "      <td>[Name: Poornima Vijayashanker\\nLocation: Palo ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nName: Anthony Kelani Location: Los...</td>\n",
       "      <td>An aspiring tech entrepreneur can leverage the...</td>\n",
       "      <td>An aspiring tech entrepreneur can learn from A...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How has being featured in Forbes and other maj...</td>\n",
       "      <td>[Name: James Creech\\nLocation: Los Angeles, Ca...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nWANT TO GET IN TOUCH? Shoot me a D...</td>\n",
       "      <td>Being featured in prestigious publications lik...</td>\n",
       "      <td>Being featured in major publications such as F...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What be the connection between OPEAR and Opera...</td>\n",
       "      <td>[Name: Poornima Vijayashanker\\nLocation: Palo ...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nHighlights: [\"fortune_500_experien...</td>\n",
       "      <td>The connection between OPEAR and Operam in ter...</td>\n",
       "      <td>The connection between OPEAR and Operam in ter...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How did Poornima Vijayashanker and Jeff Pressm...</td>\n",
       "      <td>[Name: Jeff Pressman\\nLocation: Los Angeles, C...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nSkills: [\"Software Development\",\"P...</td>\n",
       "      <td>Poornima Vijayashanker and Jeff Pressman made ...</td>\n",
       "      <td>Poornima Vijayashanker, as the Co-Founder and ...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What role did Oliver Walsh play in the growth ...</td>\n",
       "      <td>[Name: Oliver Walsh\\nLocation: Los Angeles, Ca...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nName: Oliver Walsh Location: Los A...</td>\n",
       "      <td>Oliver Walsh played a crucial role in the grow...</td>\n",
       "      <td>Oliver Walsh served as the CMO and Board Direc...</td>\n",
       "      <td>multi_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What role does the University of Kent in Ameri...   \n",
       "1              What Keith Teare do in United States?   \n",
       "2  how taiwan fit into the strategy for realnames...   \n",
       "3  what u know bout University of Kent and what k...   \n",
       "4  As an International Finance and Strategy Exper...   \n",
       "5  How can an aspiring tech entrepreneur leverage...   \n",
       "6  How has being featured in Forbes and other maj...   \n",
       "7  What be the connection between OPEAR and Opera...   \n",
       "8  How did Poornima Vijayashanker and Jeff Pressm...   \n",
       "9  What role did Oliver Walsh play in the growth ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Name: Christian Busch\\nLocation: Los Angeles,...   \n",
       "1  [Name: Keith Teare\\nLocation: Palo Alto, Calif...   \n",
       "2  [Name: Alan Chiu\\nLocation: Palo Alto, Califor...   \n",
       "3  [Name: Keith Teare\\nLocation: Palo Alto, Calif...   \n",
       "4  [Name: Keith Teare\\nLocation: Palo Alto, Calif...   \n",
       "5  [Name: Poornima Vijayashanker\\nLocation: Palo ...   \n",
       "6  [Name: James Creech\\nLocation: Los Angeles, Ca...   \n",
       "7  [Name: Poornima Vijayashanker\\nLocation: Palo ...   \n",
       "8  [Name: Jeff Pressman\\nLocation: Los Angeles, C...   \n",
       "9  [Name: Oliver Walsh\\nLocation: Los Angeles, Ca...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Experience\",\"Consumer Internet\",\"Digital Mark...   \n",
       "1  [Name: Keith Teare Location: Palo Alto, Califo...   \n",
       "2  [about the IP Communications space, and partic...   \n",
       "3  [Education\",\"Hardware\",\"Internet Services\",\"Te...   \n",
       "4  [Experience: [{\"Company Name\":\"Start-up-Chris-...   \n",
       "5  [<1-hop>\\n\\nName: Anthony Kelani Location: Los...   \n",
       "6  [<1-hop>\\n\\nWANT TO GET IN TOUCH? Shoot me a D...   \n",
       "7  [<1-hop>\\n\\nHighlights: [\"fortune_500_experien...   \n",
       "8  [<1-hop>\\n\\nSkills: [\"Software Development\",\"P...   \n",
       "9  [<1-hop>\\n\\nName: Oliver Walsh Location: Los A...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The University of Kent in America plays a role...   \n",
       "1  Keith Teare is based in Palo Alto, California,...   \n",
       "2  Taiwan fit into the strategy for RealNames dur...   \n",
       "3  Keith Teare has an honorary doctorate from the...   \n",
       "4  Polkadot significantly contributes to the adva...   \n",
       "5  An aspiring tech entrepreneur can leverage the...   \n",
       "6  Being featured in prestigious publications lik...   \n",
       "7  The connection between OPEAR and Operam in ter...   \n",
       "8  Poornima Vijayashanker and Jeff Pressman made ...   \n",
       "9  Oliver Walsh played a crucial role in the grow...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  The individual serves as a volunteer director ...   \n",
       "1  Keith Teare is a founder and CEO at SignalRank...   \n",
       "2  RealNames made deals across 12 countries, incl...   \n",
       "3  Keith Teare was presented with an honorary doc...   \n",
       "4  Polkadot is a blockspace ecosystem designed fo...   \n",
       "5  An aspiring tech entrepreneur can learn from A...   \n",
       "6  Being featured in major publications such as F...   \n",
       "7  The connection between OPEAR and Operam in ter...   \n",
       "8  Poornima Vijayashanker, as the Co-Founder and ...   \n",
       "9  Oliver Walsh served as the CMO and Board Direc...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  single_hop_specifc_query_synthesizer  \n",
       "5  multi_hop_specific_query_synthesizer  \n",
       "6  multi_hop_specific_query_synthesizer  \n",
       "7  multi_hop_specific_query_synthesizer  \n",
       "8  multi_hop_specific_query_synthesizer  \n",
       "9  multi_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdff46b822834956a59d3fff72f5e280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[1]: TimeoutError()\n",
      "Exception raised in Job[4]: TimeoutError()\n",
      "Exception raised in Job[6]: TimeoutError()\n",
      "Exception raised in Job[7]: TimeoutError()\n",
      "Exception raised in Job[10]: TimeoutError()\n",
      "Exception raised in Job[13]: TimeoutError()\n",
      "Exception raised in Job[11]: TimeoutError()\n",
      "Exception raised in Job[12]: TimeoutError()\n",
      "Exception raised in Job[16]: TimeoutError()\n",
      "Exception raised in Job[18]: TimeoutError()\n",
      "Exception raised in Job[19]: TimeoutError()\n",
      "Exception raised in Job[22]: TimeoutError()\n",
      "Exception raised in Job[24]: TimeoutError()\n",
      "Exception raised in Job[25]: TimeoutError()\n",
      "Exception raised in Job[28]: TimeoutError()\n",
      "Exception raised in Job[29]: TimeoutError()\n",
      "Exception raised in Job[30]: TimeoutError()\n",
      "Exception raised in Job[34]: TimeoutError()\n",
      "Exception raised in Job[31]: TimeoutError()\n",
      "Exception raised in Job[36]: TimeoutError()\n",
      "Exception raised in Job[37]: TimeoutError()\n",
      "Exception raised in Job[35]: TimeoutError()\n",
      "Exception raised in Job[40]: TimeoutError()\n",
      "Exception raised in Job[41]: TimeoutError()\n",
      "Exception raised in Job[42]: TimeoutError()\n",
      "Exception raised in Job[43]: TimeoutError()\n",
      "Exception raised in Job[46]: TimeoutError()\n",
      "Exception raised in Job[48]: TimeoutError()\n",
      "Exception raised in Job[49]: TimeoutError()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 1.0000, 'faithfulness': 0.7812, 'factual_correctness': 0.4720, 'answer_relevancy': 0.9383, 'context_entity_recall': 0.4722, 'noise_sensitivity_relevant': 0.4856}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_dataset,\n",
    "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
